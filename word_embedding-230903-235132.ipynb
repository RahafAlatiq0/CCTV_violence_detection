{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Word Embedding:  word2vec VS GloVe\n"
      ],
      "metadata": {
        "id": "VPt9t4aBvQg1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Word Embedding using word2vec"
      ],
      "metadata": {
        "id": "C6LqQ34ywABH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word2Vec is a feed forward neural network based model to find word embeddings."
      ],
      "metadata": {
        "id": "pG-u2A1a3qGO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5oU0vIJ24FKZ",
        "outputId": "b9efec77-eb04-45cc-bc49-af13b42e9dd9"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "import numpy as np\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "id": "ff0I8NY0vbDW"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = [\n",
        "    \"I love natural language processing\",\n",
        "    \"Word embeddings are useful for NLP tasks\",\n",
        "    \"Word2Vec and GloVe are popular word embedding techniques\",\n",
        "    \"Embeddings capture semantic relationships between words\"\n",
        "]"
      ],
      "metadata": {
        "id": "0SUc7kV4wDjg"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Corpus is a list that contains a collection of text sentences or phrases. Each element in the list represents a single sentence or phrase. This corpus can be considered a small collection of text used for demonstration or experimentation purposes in NLP tasks"
      ],
      "metadata": {
        "id": "E5-1yx9r4OJY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_corpus = [word_tokenize(sentence.lower()) for sentence in corpus]"
      ],
      "metadata": {
        "id": "rKunPPXRwI-_"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creates a tokenized version of the original corpus, breaking down each sentence into its individual words."
      ],
      "metadata": {
        "id": "4BsBSODF4vUv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_corpus"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2OI4I4WRwJfz",
        "outputId": "608e35e0-8ed6-4003-b09e-0db60b249d0f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['i', 'love', 'natural', 'language', 'processing'],\n",
              " ['word', 'embeddings', 'are', 'useful', 'for', 'nlp', 'tasks'],\n",
              " ['word2vec',\n",
              "  'and',\n",
              "  'glove',\n",
              "  'are',\n",
              "  'popular',\n",
              "  'word',\n",
              "  'embedding',\n",
              "  'techniques'],\n",
              " ['embeddings', 'capture', 'semantic', 'relationships', 'between', 'words']]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Word2Vec(sentences=tokenized_corpus, vector_size=100, window=5, min_count=1, sg=0)"
      ],
      "metadata": {
        "id": "AfU7ABerwKae"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word2Vec model is being created and trained using the provided **tokenized_corpus**.\n",
        "\n",
        "We are using the Skip-gram architecuture here, which is specified using \"sg=0\""
      ],
      "metadata": {
        "id": "nsDP5iNY5Gc4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vector = model.wv['love']\n",
        "\n",
        "print(\"Word vector for 'love':\", vector)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VXRz2aSYwsne",
        "outputId": "2cd9a941-afa9-48a9-defe-d8fecbe34a4a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word vector for 'love': [-8.7276660e-03  2.1322442e-03 -8.7234029e-04 -9.3180090e-03\n",
            " -9.4277617e-03 -1.4115345e-03  4.4360980e-03  3.7051938e-03\n",
            " -6.4997668e-03 -6.8744812e-03 -4.9965675e-03 -2.2889164e-03\n",
            " -7.2506540e-03 -9.6036931e-03 -2.7440847e-03 -8.3625512e-03\n",
            " -6.0377736e-03 -5.6711119e-03 -2.3457278e-03 -1.7099340e-03\n",
            " -8.9553650e-03 -7.3277560e-04  8.1559829e-03  7.6896204e-03\n",
            " -7.2059133e-03 -3.6662850e-03  3.1182212e-03 -9.5726410e-03\n",
            "  1.4767149e-03  6.5224483e-03  5.7462831e-03 -8.7638423e-03\n",
            " -4.5154700e-03 -8.1404923e-03  4.4413522e-05  9.2636719e-03\n",
            "  5.9747146e-03  5.0679548e-03  5.0612800e-03 -3.2449535e-03\n",
            "  9.5528029e-03 -7.3563098e-03 -7.2702118e-03 -2.2664559e-03\n",
            " -7.7782269e-04 -3.2147393e-03 -5.9475785e-04  7.4897148e-03\n",
            " -6.9789623e-04 -1.6242372e-03  2.7439173e-03 -8.3591873e-03\n",
            "  7.8553734e-03  8.5351923e-03 -9.5840860e-03  2.4437979e-03\n",
            "  9.9072168e-03 -7.6671345e-03 -6.9674756e-03 -7.7371001e-03\n",
            "  8.3942134e-03 -6.8317266e-04  9.1467435e-03 -8.1588430e-03\n",
            "  3.7418548e-03  2.6366923e-03  7.4117485e-04  2.3284750e-03\n",
            " -7.4718576e-03 -9.3589416e-03  2.3531627e-03  6.1496398e-03\n",
            "  7.9872832e-03  5.7365033e-03 -7.7751727e-04  8.3041042e-03\n",
            " -9.3353279e-03  3.4057449e-03  2.6498904e-04  3.8570175e-03\n",
            "  7.3856395e-03 -6.7266477e-03  5.5860807e-03 -9.5224511e-03\n",
            " -8.0389873e-04 -8.6890236e-03 -5.0962684e-03  9.2881303e-03\n",
            " -1.8600362e-03  2.9151486e-03  9.0717506e-03  8.9390231e-03\n",
            " -8.2077272e-03 -3.0119431e-03  9.8898392e-03  5.1058545e-03\n",
            " -1.5880791e-03 -8.6918240e-03  2.9617434e-03 -6.6746143e-03]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "using the trained Word2Vec model, we can obtain the numerical representation for the word 'love' and then printing that vector."
      ],
      "metadata": {
        "id": "rurJYeAX5k9P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Word Embedding using GloVe"
      ],
      "metadata": {
        "id": "tAD2wLDqy1uA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GloVe is an unsupervised learning algorithm that relies on statistical properties of word co-occurrence within a corpus to learn word embeddings."
      ],
      "metadata": {
        "id": "DPR96W0s9TeW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "glove_file = \"glove.6B.100d.txt\""
      ],
      "metadata": {
        "id": "zqMYA8AVwue-"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The corpus we will use for the GloVe example"
      ],
      "metadata": {
        "id": "0SLHc1Uo5992"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings_index = {}\n",
        "with open(glove_file, encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        vector = np.array(values[1:], dtype=\"float32\")\n",
        "        embeddings_index[word] = vector\n"
      ],
      "metadata": {
        "id": "aO3fYwA0zFpV"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "you are creating a Python dictionary called embeddings_index to map words to their respective GloVe word vectors. This code processes a GloVe word vectors file to extract the word vectors and store them in the dictionary."
      ],
      "metadata": {
        "id": "RVLoB8n76CpZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word = \"love\"\n",
        "vector = embeddings_index.get(word)"
      ],
      "metadata": {
        "id": "zJzxhM43y0H0"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's test it out using the word \"love\""
      ],
      "metadata": {
        "id": "vTXVoLKZ6MJG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"GloVe vector for 'love':\", vector)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mh97P0Rk1gpR",
        "outputId": "5e2b3682-ab0e-447e-ec79-c3c6d30e7bee"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GloVe vector for 'love': [ 2.5975e-01  5.5833e-01  5.7986e-01 -2.1361e-01  1.3084e-01  9.4385e-01\n",
            " -4.2817e-01 -3.7420e-01 -9.4499e-02 -4.3344e-01 -2.0937e-01  3.4702e-01\n",
            "  8.2516e-02  7.9735e-01  1.6606e-01 -2.6878e-01  5.8830e-01  6.7397e-01\n",
            " -4.9965e-01  1.4764e+00  5.5261e-01  2.5295e-02 -1.6068e-01 -1.3878e-01\n",
            "  4.8686e-01  1.1420e+00  5.6195e-02 -7.3306e-01  8.6932e-01 -3.5892e-01\n",
            " -5.1877e-01  9.0402e-01  4.9249e-01 -1.4915e-01  4.8493e-02  2.6096e-01\n",
            "  1.1352e-01  4.1275e-01  5.3803e-01 -4.4950e-01  8.5733e-02  9.1184e-02\n",
            "  5.0177e-03 -3.4645e-01 -1.1058e-01 -2.2235e-01 -6.5290e-01 -5.1838e-02\n",
            "  5.3791e-01 -8.1040e-01 -1.8253e-01  2.4194e-01  5.4855e-01  8.7731e-01\n",
            "  2.2165e-01 -2.7124e+00  4.9405e-01  4.4703e-01  5.5882e-01  2.6076e-01\n",
            "  2.3760e-01  1.0668e+00 -5.6971e-01 -6.4960e-01  3.3511e-01  3.4609e-01\n",
            "  1.1033e+00  8.5261e-02  2.4847e-02 -4.5453e-01  7.7012e-02  2.1321e-01\n",
            "  1.0444e-01  6.7157e-02 -3.4261e-01  8.5534e-01  1.3361e-01 -4.3296e-01\n",
            " -5.6726e-01 -2.1348e-01 -3.3277e-01  3.4351e-01  3.2164e-01  4.4527e-01\n",
            " -1.3208e+00 -1.3270e-01 -7.0820e-01 -4.8472e-01 -6.9396e-01 -2.6080e-01\n",
            " -4.7099e-01 -5.7492e-02  9.3587e-02  4.0006e-01 -4.3419e-01 -2.7364e-01\n",
            " -7.7017e-01 -8.4028e-01 -1.5620e-03  6.2223e-01]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The numerical representation for the word 'love' printed as a vector."
      ],
      "metadata": {
        "id": "VQSmI5dA6PoD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GloVe can be used to calculate the similarity between two words. This is helpful for capturing semantic relationships between words."
      ],
      "metadata": {
        "id": "gNn-Wr1n8MIu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word1 = \"king\"\n",
        "word2 = \"queen\"\n",
        "\n",
        "\n",
        "# Ensure both words are in the embeddings dictionary\n",
        "if word1 in embeddings_index and word2 in embeddings_index:\n",
        "    # Calculate cosine similarity between the word vectors\n",
        "    vector1 = embeddings_index[word1]\n",
        "    vector2 = embeddings_index[word2]\n",
        "    similarity_score = cosine_similarity([vector1], [vector2])[0][0]\n",
        "    print(f\"Cosine Similarity between '{word1}' and '{word2}' using GloVe: {similarity_score:.4f}\")\n",
        "else:\n",
        "    print(\"One or both words are not in the embeddings dictionary.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zIgkeGbE8A4w",
        "outputId": "3e548861-286c-433c-a158-c66afdaaf959"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine Similarity between 'king' and 'queen' using GloVe: 0.7508\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercises:\n",
        "**1. Modify the Word2Vec example:** We have implemented the example using Skip-gram architechture, try it using Continuous Bag of Words and report your findings.\n",
        "\n",
        "**2. Calculate word similarity using Word2Vec:** Similar to GloVe,write code to calculate the cosine similarity between the two words \"queen\" and \"king\""
      ],
      "metadata": {
        "id": "JNPoa9hC6cxG"
      }
    }
  ]
}