{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Import packages"
      ],
      "metadata": {
        "id": "XcJd9UzG3I9N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install immutabledict sacrebleu sentencepiece seqeval tensorflow-model-optimization>=0.4.1 tensorflow-text~=2.13.0\n",
        "!pip install tf-models-official --no-deps --force-reinstall pyyaml>=6.0.0"
      ],
      "metadata": {
        "id": "JutXiPqOnbRu"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tqdm\n",
        "import random\n",
        "import pathlib\n",
        "import itertools\n",
        "import collections\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "from sklearn.metrics import classification_report,accuracy_score\n",
        "\n",
        "# Import the MoViNet model from TensorFlow Models (tf-models-official) for the MoViNet model\n",
        "from official.projects.movinet.modeling import movinet\n",
        "from official.projects.movinet.modeling import movinet_model"
      ],
      "metadata": {
        "id": "SIumpR5NAsB5"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import random\n",
        "from pathlib import Path\n",
        "import os\n",
        "import cv2\n",
        "import re\n",
        "import collections\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "kaggle_username = \"rahaf8\"\n",
        "kaggle_key = \"f59b8cb26f2973bc6fb4c52b1516ac19\"\n",
        "os.environ[\"KAGGLE_USERNAME\"] = kaggle_username\n",
        "os.environ[\"KAGGLE_KEY\"] = kaggle_key\n",
        "import kaggle\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "crpJ0COcY2Ok"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Define Functions"
      ],
      "metadata": {
        "id": "n4geNZG43RTE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.1 Get data from Kaggle"
      ],
      "metadata": {
        "id": "eEMfdN8E3X6H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_kaggle_dataset(dataset_name):\n",
        "    \"\"\"Get dataset from Kaggle.\n",
        "\n",
        "    Args:\n",
        "        dataset_name: the dataset name.\n",
        "    \"\"\"\n",
        "\n",
        "    # Download the dataset using the Kaggle API\n",
        "    kaggle.api.dataset_download_files(dataset_name, path=\".\", unzip=True)"
      ],
      "metadata": {
        "id": "Te69JLXQ2lh6"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.2 Video Dataframe Generator"
      ],
      "metadata": {
        "id": "tk1wraTG3qtF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def video_dataframe(data_dir):\n",
        "  \"\"\"Get video dataframe.\n",
        "\n",
        "    Args:\n",
        "      files_path: A path from which the files can be stored.\n",
        "\n",
        "    Returns:\n",
        "      Video dataframe containing the labels , videos name , and videos path.\n",
        "  \"\"\"\n",
        "  vidDf = pd.DataFrame(columns=['Label','VidName','VidPath'])\n",
        "\n",
        "  for dirname, _, filenames in os.walk(data_dir):\n",
        "      for name in filenames:\n",
        "            vidDf =  vidDf.append({'Label': re.match(r'^[^\\d_]+', name).group(),\n",
        "                                   'VidName': name,\n",
        "                                   'VidPath': os.path.join(dirname, name)},\n",
        "                                    ignore_index=True)\n",
        "  return vidDf"
      ],
      "metadata": {
        "id": "rLTPe1bJL6Sb"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.3 Split Dataset"
      ],
      "metadata": {
        "id": "pVe-7dpk4e1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def SplitData(testsize, df, classes):\n",
        "    min_samples_per_class = min(df.groupby(\"Label\").size())\n",
        "    print(f\"{min_samples_per_class} Samples per Class\")\n",
        "\n",
        "    df_TrainingSet = pd.DataFrame(columns=df.columns)\n",
        "    df_TestSet = pd.DataFrame(columns=df.columns)\n",
        "\n",
        "    for class_label in classes:\n",
        "        df_class = df[df['Label'] == class_label].sample(min_samples_per_class, random_state=42)\n",
        "\n",
        "        training_set, test_set = train_test_split(df_class, test_size=testsize, random_state=42)\n",
        "\n",
        "        df_TrainingSet = df_TrainingSet.append(training_set)\n",
        "        df_TestSet = df_TestSet.append(test_set)\n",
        "\n",
        "    df_TrainingSet = df_TrainingSet.sample(frac=1, random_state=42)\n",
        "    df_TestSet = df_TestSet.sample(frac=1, random_state=42)\n",
        "\n",
        "    return df_TrainingSet, df_TestSet"
      ],
      "metadata": {
        "id": "YuHIZ_YSCDSr"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.3 Move video into train and test folder"
      ],
      "metadata": {
        "id": "Gl3vRNSX8r0t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "def delete_empty_folders(directory):\n",
        "    for root, dirs, files in os.walk(directory, topdown=False):\n",
        "        for dir_name in dirs:\n",
        "            folder_path = os.path.join(root, dir_name)\n",
        "            delete_empty_folders(folder_path)  # Recursively check subdirectories\n",
        "        if not os.listdir(root) and not files:\n",
        "            os.rmdir(root)\n",
        "\n",
        "def move_videos_to_folders(df, destination_dir):\n",
        "    for _, row in df.iterrows():\n",
        "        label = row['Label']\n",
        "        vid_name = row['VidName']\n",
        "        vid_path = row['VidPath']\n",
        "\n",
        "        folder_path = os.path.join(destination_dir, label)\n",
        "        os.makedirs(folder_path, exist_ok=True)\n",
        "\n",
        "        new_vid_path = os.path.join(folder_path, vid_name)\n",
        "        shutil.move(vid_path, new_vid_path)\n",
        "\n",
        "        df.loc[df['VidPath'] == vid_path, 'VidPath'] = new_vid_path"
      ],
      "metadata": {
        "id": "hy_dbj0W5Ogp"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.4 Find minimal frame count"
      ],
      "metadata": {
        "id": "10ZSJu5I6zGz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_minimal_frame_count(file_name):\n",
        "    with open(file_name, 'r') as file:\n",
        "        annotations = file.readlines()\n",
        "\n",
        "    minimal_value = float('inf')\n",
        "    for annotation in annotations:\n",
        "        video_info = annotation.split()\n",
        "        start_frame = int(video_info[2])\n",
        "        end_frame = int(video_info[3])\n",
        "        frame_count = end_frame - start_frame\n",
        "\n",
        "        if frame_count > 0:\n",
        "            minimal_value = min(minimal_value, frame_count)\n",
        "\n",
        "    if minimal_value == float('inf'):\n",
        "        minimal_value = 0\n",
        "\n",
        "    return minimal_value"
      ],
      "metadata": {
        "id": "ZkcYTa476O7d"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.4 This code has been copied from tensorflow website without any changes - task for tomorrow"
      ],
      "metadata": {
        "id": "l2TYEqbV9cGv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def format_frames(frame, output_size):\n",
        "  \"\"\"\n",
        "    Pad and resize an image from a video.\n",
        "\n",
        "    Args:\n",
        "      frame: Image that needs to resized and padded.\n",
        "      output_size: Pixel size of the output frame image.\n",
        "\n",
        "    Return:\n",
        "      Formatted frame with padding of specified output size.\n",
        "  \"\"\"\n",
        "  frame = tf.image.convert_image_dtype(frame, tf.float32)\n",
        "  frame = tf.image.resize_with_pad(frame, *output_size)\n",
        "  return frame"
      ],
      "metadata": {
        "id": "Aei4C2ChF-Kz"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def get_start_frame_from_annotation(annotation_file, video_path):\n",
        "    \"\"\"\n",
        "    Retrieves the starting frame from the annotation file.\n",
        "\n",
        "    Args:\n",
        "        annotation_file: File path to the annotation file.\n",
        "        video_path: File path to the video.\n",
        "\n",
        "    Return:\n",
        "        The starting frame for the video.\n",
        "    \"\"\"\n",
        "    #print(re.search(r'([^/]+)\\.mp4$', video_path).group(1))\n",
        "    video_name = re.search(r'[^/]+$', video_path).group(0)\n",
        "\n",
        "    starting_frame = 0\n",
        "    with open(annotation_file, 'r') as file:\n",
        "        for line in file:\n",
        "            video_info = line.strip().split()\n",
        "            if len(video_info) >= 3 and video_info[0] == video_name:\n",
        "                starting_frame = int(video_info[2])\n",
        "                if(starting_frame == -1):\n",
        "                  starting_frame = 0\n",
        "    return starting_frame"
      ],
      "metadata": {
        "id": "dhMYVl2kpLCw"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def frames_from_video_file(video_path, n_frames, output_size=(320, 320)):\n",
        "    \"\"\"\n",
        "    Creates frames from each video file present for each category.\n",
        "\n",
        "    Args:\n",
        "        video_path: File path to the video.\n",
        "        n_frames: Number of frames to be created per video file.\n",
        "        output_size: Pixel size of the output frame image.\n",
        "\n",
        "    Return:\n",
        "        A NumPy array of frames in the shape of (n_frames, height, width, channels).\n",
        "    \"\"\"\n",
        "    annotation_file = '/content/dataset/Filtered_Anomaly_Annotation.txt'\n",
        "\n",
        "    result = []\n",
        "    src = cv2.VideoCapture(str(video_path))\n",
        "\n",
        "    video_length = src.get(cv2.CAP_PROP_FRAME_COUNT)\n",
        "\n",
        "    need_length = n_frames\n",
        "\n",
        "    start = get_start_frame_from_annotation(annotation_file, str(video_path))\n",
        "\n",
        "    src.set(cv2.CAP_PROP_POS_FRAMES, start)\n",
        "\n",
        "    ret, frame = src.read()\n",
        "    result.append(format_frames(frame, output_size))\n",
        "\n",
        "    for _ in range(n_frames - 1):\n",
        "        ret, frame = src.read()\n",
        "        if ret:\n",
        "            frame = format_frames(frame, output_size)\n",
        "            result.append(frame)\n",
        "        else:\n",
        "            result.append(np.zeros_like(result[0]))\n",
        "\n",
        "    src.release()\n",
        "    result = np.array(result)[..., [2, 1, 0]]\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "9PqFNA_AnaDX"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_frames(path, df, n_frames, training=False):\n",
        "    pairs = list(zip(df['VidPath'], df['Label']))\n",
        "    class_names = df['Label'].unique().tolist()\n",
        "    class_ids_for_name = {name: idx for idx, name in enumerate(class_names)}\n",
        "    print(class_ids_for_name)\n",
        "\n",
        "    if training:\n",
        "        random.shuffle(pairs)\n",
        "\n",
        "    for path, name in pairs:\n",
        "        video_frames = frames_from_video_file(path, n_frames)\n",
        "        label = class_ids_for_name[name]\n",
        "        yield video_frames, label"
      ],
      "metadata": {
        "id": "bFD99lxAJsFb"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Data Preprocessing"
      ],
      "metadata": {
        "id": "ssxWAkKp4O-l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install opendatasets --quiet\n",
        "\n",
        "\n",
        "import opendatasets as od\n",
        "\n",
        "#{\"username\":\"rahaf8\",\"key\":\"f59b8cb26f2973bc6fb4c52b1516ac19\"}\n",
        "\n",
        "dataset_url = 'https://www.kaggle.com/datasets/saharyatimi/dataset'\n",
        "\n",
        "# Specify the folder or file you want to download\n",
        "\n",
        "od.download(dataset_url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BC00mIsLJRQv",
        "outputId": "abd47abf-5a21-422f-ea79-1beb1081f7d7"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n",
            "Your Kaggle username: rahaf8\n",
            "Your Kaggle Key: ··········\n",
            "Downloading dataset.zip to ./dataset\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 552M/552M [00:11<00:00, 52.5MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_name = \"saharyatimi/datasett\"\n",
        "dataset_dir = \"/content/dataset/dataset\"\n",
        "\n",
        "#get_kaggle_dataset(dataset_name)"
      ],
      "metadata": {
        "id": "VsdinTHK4R9Z"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "video_df = video_dataframe(dataset_dir)\n",
        "classes = video_df['Label'].unique().tolist()\n",
        "print('Number of classes', len(classes))\n",
        "print('Num videos for each class: : ')\n",
        "print(video_df['Label'].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x6e8i--rehke",
        "outputId": "50acd324-f438-46d5-addb-b84102bf1d23"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of classes 8\n",
            "Num videos for each class: : \n",
            "Stealing       5\n",
            "Vandalism      5\n",
            "Shooting       5\n",
            "Normal         5\n",
            "Robbery        5\n",
            "Shoplifting    5\n",
            "Burglary       5\n",
            "Fighting       5\n",
            "Name: Label, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df, test_df = SplitData(0.2, video_df,classes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xwLcswetdIt",
        "outputId": "3f13362c-5eba-4212-ab39-0bc784e0574b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5 Samples per Class\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ddata = {\"Training\":train_df.groupby(\"Label\").size(),\"Test\":test_df.groupby(\"Label\").size()}\n",
        "\n",
        "ddataframe = pd.DataFrame(data=ddata)\n",
        "ddataframe.plot.bar(stacked= True, rot= 15, title='Training vs Test data',figsize=(15,5))\n",
        "plt.show(block= True)"
      ],
      "metadata": {
        "id": "YVVxLD9721gL"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_destination_dir = '/content/dataset/train'\n",
        "test_destination_dir = '/content/dataset/test'\n",
        "\n",
        "# Move videos to train folders\n",
        "move_videos_to_folders(train_df, train_destination_dir)\n",
        "\n",
        "# Move videos to test folders\n",
        "move_videos_to_folders(test_df, test_destination_dir)\n",
        "\n",
        "#delete empty folders\n",
        "delete_empty_folders('/content/dataset/dataset')"
      ],
      "metadata": {
        "id": "NfbOa2X_6KXs"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_name = '/content/dataset/Filtered_Anomaly_Annotation.txt'\n",
        "batch_size = 8\n",
        "num_frames =  get_minimal_frame_count(file_name)\n",
        "\n",
        "output_signature = (tf.TensorSpec(shape = (None, None, None, 3), dtype = tf.float32),\n",
        "                    tf.TensorSpec(shape = (), dtype = tf.int16))\n",
        "\n",
        "train_ds = tf.data.Dataset.from_generator(\n",
        "    lambda: generate_frames(Path(train_destination_dir), train_df, num_frames, training=True),\n",
        "    output_signature=output_signature\n",
        ")\n",
        "train_ds = train_ds.batch(batch_size)\n",
        "\n",
        "test_ds = tf.data.Dataset.from_generator(\n",
        "    lambda: generate_frames(Path(test_destination_dir), test_df, num_frames),\n",
        "    output_signature=output_signature\n",
        ")\n",
        "test_ds = test_ds.batch(batch_size)"
      ],
      "metadata": {
        "id": "vsWJhNY1292p"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for frames, labels in test_ds.take(5):\n",
        "  print(labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Mo7GDl8-uBD",
        "outputId": "1adede02-5098-40c8-da95-1d02c1cb3640"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Vandalism': 0, 'Shoplifting': 1, 'Stealing': 2, 'Fighting': 3, 'Shooting': 4, 'Robbery': 5, 'Normal': 6, 'Burglary': 7}\n",
            "tf.Tensor([0 1 2 3 4 5 6 7], shape=(8,), dtype=int16)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Shape: {frames.shape}\")\n",
        "print(f\"Label: {labels.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59LH2qwD-3lB",
        "outputId": "d0da3df0-8ce5-4aee-922a-4b962f5f2fe4"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape: (8, 60, 320, 320, 3)\n",
            "Label: (8,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import PIL\n",
        "def tensor_to_image(tensor):\n",
        "    tensor = tensor*255\n",
        "    tensor = np.array(tensor, dtype=np.uint8)\n",
        "    if np.ndim(tensor)>3:\n",
        "        assert tensor.shape[0] == 1\n",
        "        tensor = tensor[0]\n",
        "    return tensor"
      ],
      "metadata": {
        "id": "2CA6iymNJAYE"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "# Retrieve one batch of frames from the train_ds dataset\n",
        "batch = next(iter(test_ds))\n",
        "\n",
        "# Unpack the batch into frames and labels\n",
        "frames, labels = batch\n",
        "\n",
        "# Select the first frame from the frames tensor\n",
        "frame = frames[0, 0]  # Assuming the batch dimension is the first dimension\n",
        "\n",
        "\n",
        "# Create a list to hold the frames\n",
        "frames2 = []\n",
        "\n",
        "# Convert each NumPy array to a PIL Image and append to the frames list\n",
        "for i in range(0,60):\n",
        "    frames2.append(Image.fromarray(tensor_to_image(frames[0, i])))\n",
        "\n",
        "\n",
        "# Create a GIF file with the frames\n",
        "output_file = 'output.gif'\n",
        "frames2[0].save(output_file, save_all=True, append_images=frames2[1:], loop=0, duration=200)\n",
        "\n",
        "print(f\"Animated GIF saved as '{output_file}'\")"
      ],
      "metadata": {
        "id": "RZfnIBcuJb2f",
        "outputId": "c852dcff-be3d-4ca0-e501-ccc76ff4f7f3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Vandalism': 0, 'Shoplifting': 1, 'Stealing': 2, 'Fighting': 3, 'Shooting': 4, 'Robbery': 5, 'Normal': 6, 'Burglary': 7}\n",
            "Animated GIF saved as 'output.gif'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## the code below is copied from tensorflow website without any changes - task for tomorrow"
      ],
      "metadata": {
        "id": "bIUEHjiFB6hx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = 'a5'\n",
        "resolution = 320\n",
        "\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "backbone = movinet.Movinet(model_id=model_id)\n",
        "backbone.trainable = False\n",
        "\n",
        "# Set num_classes=600 to load the pre-trained weights from the original model\n",
        "model = movinet_model.MovinetClassifier(backbone=backbone, num_classes=600)\n",
        "model.build([None, None, None, None, 3])\n",
        "\n",
        "# Load pre-trained weights\n",
        "!wget https://storage.googleapis.com/tf_model_garden/vision/movinet/movinet_a0_base.tar.gz -O movinet_a0_base.tar.gz -q\n",
        "!tar -xvf movinet_a0_base.tar.gz\n",
        "\n",
        "checkpoint_dir = f'movinet_{model_id}_base'\n",
        "checkpoint_path = tf.train.latest_checkpoint(checkpoint_dir)\n",
        "checkpoint = tf.train.Checkpoint(model=model)\n",
        "status = checkpoint.restore(checkpoint_path)\n",
        "status.assert_existing_objects_matched()"
      ],
      "metadata": {
        "id": "D7VoRSxHB-2c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_classifier(batch_size, num_frames, resolution, backbone, num_classes):\n",
        "  \"\"\"Builds a classifier on top of a backbone model.\"\"\"\n",
        "  model = movinet_model.MovinetClassifier(\n",
        "      backbone=backbone,\n",
        "      num_classes=num_classes)\n",
        "  model.build([batch_size, num_frames, resolution, resolution, 3])\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "oJ4GS9QTCA4r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_classifier(batch_size, num_frames, resolution, backbone, 8)"
      ],
      "metadata": {
        "id": "4QGjks51CCIC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 15\n",
        "\n",
        "loss_obj = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate = 0.01)\n",
        "\n",
        "model.compile(loss=loss_obj, optimizer=optimizer, metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "IC2TAm69CDoZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = model.fit(train_ds,\n",
        "                    validation_data=test_ds,\n",
        "                    epochs=num_epochs,\n",
        "                    validation_freq=1,\n",
        "                    verbose=1)"
      ],
      "metadata": {
        "id": "1-WlgpvoCFVa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(test_ds, return_dict=True)"
      ],
      "metadata": {
        "id": "W3BlNoCICIme"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_actual_predicted_labels(dataset):\n",
        "  \"\"\"\n",
        "    Create a list of actual ground truth values and the predictions from the model.\n",
        "\n",
        "    Args:\n",
        "      dataset: An iterable data structure, such as a TensorFlow Dataset, with features and labels.\n",
        "\n",
        "    Return:\n",
        "      Ground truth and predicted values for a particular dataset.\n",
        "  \"\"\"\n",
        "  actual = [labels for _, labels in dataset.unbatch()]\n",
        "  predicted = model.predict(dataset)\n",
        "\n",
        "  actual = tf.stack(actual, axis=0)\n",
        "  predicted = tf.concat(predicted, axis=0)\n",
        "  predicted = tf.argmax(predicted, axis=1)\n",
        "\n",
        "  return actual, predicted"
      ],
      "metadata": {
        "id": "k3RE_h_wCLfo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_confusion_matrix(actual, predicted, labels, ds_type):\n",
        "  cm = tf.math.confusion_matrix(actual, predicted)\n",
        "  ax = sns.heatmap(cm, annot=True, fmt='g')\n",
        "  sns.set(rc={'figure.figsize':(12, 12)})\n",
        "  sns.set(font_scale=1.4)\n",
        "  ax.set_title('Confusion matrix of action recognition for ' + ds_type)\n",
        "  ax.set_xlabel('Predicted Action')\n",
        "  ax.set_ylabel('Actual Action')\n",
        "  plt.xticks(rotation=90)\n",
        "  plt.yticks(rotation=0)\n",
        "  ax.xaxis.set_ticklabels(labels)\n",
        "  ax.yaxis.set_ticklabels(labels)"
      ],
      "metadata": {
        "id": "eqzk3JWmKG4X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fg = FrameGenerator(Path(train_destination_dir), num_frames, training = True)\n",
        "label_names = list(fg.class_ids_for_name.keys())"
      ],
      "metadata": {
        "id": "4f2al16ECQi-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "actual, predicted = get_actual_predicted_labels(test_ds)"
      ],
      "metadata": {
        "id": "B1l5S1kVCR7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'MobileNet Model accuracy on the test set is : {accuracy_score(actual, predicted )*100:.2f}%')"
      ],
      "metadata": {
        "id": "pFh8oMy7LlCV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dphxtosHDDtJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dqKaxgeKhEka"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}