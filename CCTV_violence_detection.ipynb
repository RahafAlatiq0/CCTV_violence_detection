{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Import packages"
      ],
      "metadata": {
        "id": "XcJd9UzG3I9N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install immutabledict sacrebleu sentencepiece seqeval tensorflow-model-optimization>=0.4.1 tensorflow-text~=2.13.0\n",
        "!pip install tf-models-official --no-deps --force-reinstall pyyaml>=6.0.0"
      ],
      "metadata": {
        "id": "JutXiPqOnbRu"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tqdm\n",
        "import random\n",
        "import pathlib\n",
        "import itertools\n",
        "import collections\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "from sklearn.metrics import classification_report,accuracy_score\n",
        "\n",
        "# Import the MoViNet model from TensorFlow Models (tf-models-official) for the MoViNet model\n",
        "from official.projects.movinet.modeling import movinet\n",
        "from official.projects.movinet.modeling import movinet_model"
      ],
      "metadata": {
        "id": "SIumpR5NAsB5"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import random\n",
        "from pathlib import Path\n",
        "import os\n",
        "import cv2\n",
        "import re\n",
        "import collections\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "kaggle_username = \"rahaf8\"\n",
        "kaggle_key = \"f59b8cb26f2973bc6fb4c52b1516ac19\"\n",
        "os.environ[\"KAGGLE_USERNAME\"] = kaggle_username\n",
        "os.environ[\"KAGGLE_KEY\"] = kaggle_key\n",
        "import kaggle\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "crpJ0COcY2Ok"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Define Functions"
      ],
      "metadata": {
        "id": "n4geNZG43RTE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.1 Get data from Kaggle"
      ],
      "metadata": {
        "id": "eEMfdN8E3X6H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_kaggle_dataset(dataset_name):\n",
        "    \"\"\"Get dataset from Kaggle.\n",
        "\n",
        "    Args:\n",
        "        dataset_name: the dataset name.\n",
        "    \"\"\"\n",
        "\n",
        "    # Download the dataset using the Kaggle API\n",
        "    kaggle.api.dataset_download_files(dataset_name, path=\".\", unzip=True)"
      ],
      "metadata": {
        "id": "Te69JLXQ2lh6"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.2 Video Dataframe Generator"
      ],
      "metadata": {
        "id": "tk1wraTG3qtF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def video_dataframe(data_dir):\n",
        "  \"\"\"Get video dataframe.\n",
        "\n",
        "    Args:\n",
        "      files_path: A path from which the files can be stored.\n",
        "\n",
        "    Returns:\n",
        "      Video dataframe containing the labels , videos name , and videos path.\n",
        "  \"\"\"\n",
        "  vidDf = pd.DataFrame(columns=['Label','VidName','VidPath'])\n",
        "\n",
        "  for dirname, _, filenames in os.walk(data_dir):\n",
        "      for name in filenames:\n",
        "            vidDf =  vidDf.append({'Label': re.match(r'^[^\\d_]+', name).group(),\n",
        "                                   'VidName': name,\n",
        "                                   'VidPath': os.path.join(dirname, name)},\n",
        "                                    ignore_index=True)\n",
        "  return vidDf"
      ],
      "metadata": {
        "id": "rLTPe1bJL6Sb"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.3 Split Dataset"
      ],
      "metadata": {
        "id": "pVe-7dpk4e1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def SplitData(testsize, df, classes):\n",
        "    min_samples_per_class = min(df.groupby(\"Label\").size())\n",
        "    print(f\"{min_samples_per_class} Samples per Class\")\n",
        "\n",
        "    df_TrainingSet = pd.DataFrame(columns=df.columns)\n",
        "    df_TestSet = pd.DataFrame(columns=df.columns)\n",
        "\n",
        "    for class_label in classes:\n",
        "        df_class = df[df['Label'] == class_label].sample(min_samples_per_class, random_state=42)\n",
        "\n",
        "        training_set, test_set = train_test_split(df_class, test_size=testsize, random_state=42)\n",
        "\n",
        "        df_TrainingSet = df_TrainingSet.append(training_set)\n",
        "        df_TestSet = df_TestSet.append(test_set)\n",
        "\n",
        "    df_TrainingSet = df_TrainingSet.sample(frac=1, random_state=42)\n",
        "    df_TestSet = df_TestSet.sample(frac=1, random_state=42)\n",
        "\n",
        "    return df_TrainingSet, df_TestSet"
      ],
      "metadata": {
        "id": "YuHIZ_YSCDSr"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.3 Move video into train and test folder"
      ],
      "metadata": {
        "id": "Gl3vRNSX8r0t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "def delete_empty_folders(directory):\n",
        "    for root, dirs, files in os.walk(directory, topdown=False):\n",
        "        for dir_name in dirs:\n",
        "            folder_path = os.path.join(root, dir_name)\n",
        "            delete_empty_folders(folder_path)  # Recursively check subdirectories\n",
        "        if not os.listdir(root) and not files:\n",
        "            os.rmdir(root)\n",
        "\n",
        "def move_videos_to_folders(df, destination_dir):\n",
        "    for _, row in df.iterrows():\n",
        "        label = row['Label']\n",
        "        vid_name = row['VidName']\n",
        "        vid_path = row['VidPath']\n",
        "\n",
        "        folder_path = os.path.join(destination_dir, label)\n",
        "        os.makedirs(folder_path, exist_ok=True)\n",
        "\n",
        "        new_vid_path = os.path.join(folder_path, vid_name)\n",
        "        shutil.move(vid_path, new_vid_path)\n",
        "\n",
        "        df.loc[df['VidPath'] == vid_path, 'VidPath'] = new_vid_path"
      ],
      "metadata": {
        "id": "hy_dbj0W5Ogp"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.4 Find minimal frame count"
      ],
      "metadata": {
        "id": "10ZSJu5I6zGz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_minimal_frame_count(file_name):\n",
        "    with open(file_name, 'r') as file:\n",
        "        annotations = file.readlines()\n",
        "\n",
        "    minimal_value = float('inf')\n",
        "    for annotation in annotations:\n",
        "        video_info = annotation.split()\n",
        "        start_frame = int(video_info[2])\n",
        "        end_frame = int(video_info[3])\n",
        "        frame_count = end_frame - start_frame\n",
        "        if frame_count > 0:\n",
        "            minimal_value = min(minimal_value, frame_count)\n",
        "\n",
        "    if minimal_value == float('inf'):\n",
        "        minimal_value = 0\n",
        "\n",
        "    return minimal_value"
      ],
      "metadata": {
        "id": "ZkcYTa476O7d"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.4 This code has been copied from tensorflow website without any changes - task for tomorrow"
      ],
      "metadata": {
        "id": "l2TYEqbV9cGv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def format_frames(frame, output_size):\n",
        "  \"\"\"\n",
        "    Pad and resize an image from a video.\n",
        "\n",
        "    Args:\n",
        "      frame: Image that needs to resized and padded.\n",
        "      output_size: Pixel size of the output frame image.\n",
        "\n",
        "    Return:\n",
        "      Formatted frame with padding of specified output size.\n",
        "  \"\"\"\n",
        "  frame = tf.image.convert_image_dtype(frame, tf.float32)\n",
        "  frame = tf.image.resize_with_pad(frame, *output_size)\n",
        "  return frame"
      ],
      "metadata": {
        "id": "Aei4C2ChF-Kz"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def frames_from_video_file(video_path, n_frames, output_size = (320,320), frame_step = 15):\n",
        "  \"\"\"\n",
        "    Creates frames from each video file present for each category.\n",
        "\n",
        "    Args:\n",
        "      video_path: File path to the video.\n",
        "      n_frames: Number of frames to be created per video file.\n",
        "      output_size: Pixel size of the output frame image.\n",
        "\n",
        "    Return:\n",
        "      An NumPy array of frames in the shape of (n_frames, height, width, channels).\n",
        "  \"\"\"\n",
        "  # Read each video frame by frame\n",
        "  result = []\n",
        "  src = cv2.VideoCapture(str(video_path))\n",
        "\n",
        "  video_length = src.get(cv2.CAP_PROP_FRAME_COUNT)\n",
        "\n",
        "  need_length = 1 + (n_frames - 1) * frame_step\n",
        "\n",
        "  if need_length > video_length:\n",
        "    start = 0\n",
        "  else:\n",
        "    max_start = video_length - need_length\n",
        "    start = random.randint(0, max_start + 1)\n",
        "\n",
        "  src.set(cv2.CAP_PROP_POS_FRAMES, start)\n",
        "  # ret is a boolean indicating whether read was successful, frame is the image itself\n",
        "  ret, frame = src.read()\n",
        "  result.append(format_frames(frame, output_size))\n",
        "\n",
        "  for _ in range(n_frames - 1):\n",
        "    for _ in range(frame_step):\n",
        "      ret, frame = src.read()\n",
        "    if ret:\n",
        "      frame = format_frames(frame, output_size)\n",
        "      result.append(frame)\n",
        "    else:\n",
        "      result.append(np.zeros_like(result[0]))\n",
        "  src.release()\n",
        "  result = np.array(result)[..., [2, 1, 0]]\n",
        "\n",
        "  return result"
      ],
      "metadata": {
        "id": "lPvC7W3AGBTc"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_frames(path, df, n_frames, training=False):\n",
        "    pairs = list(zip(df['VidPath'], df['Label']))\n",
        "    class_names = df['Label'].unique().tolist()\n",
        "    class_ids_for_name = {name: idx for idx, name in enumerate(class_names)}\n",
        "\n",
        "    if training:\n",
        "        random.shuffle(pairs)\n",
        "\n",
        "    for path, name in pairs:\n",
        "        video_frames = frames_from_video_file(path, n_frames)\n",
        "        label = class_ids_for_name[name]\n",
        "        yield video_frames, label"
      ],
      "metadata": {
        "id": "bFD99lxAJsFb"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Data Preprocessing"
      ],
      "metadata": {
        "id": "ssxWAkKp4O-l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install opendatasets --quiet\n",
        "\n",
        "\n",
        "import opendatasets as od\n",
        "\n",
        "#{\"username\":\"rahaf8\",\"key\":\"f59b8cb26f2973bc6fb4c52b1516ac19\"}\n",
        "\n",
        "dataset_url = 'https://www.kaggle.com/datasets/saharyatimi/dataset'\n",
        "\n",
        "# Specify the folder or file you want to download\n",
        "\n",
        "od.download(dataset_url)"
      ],
      "metadata": {
        "id": "BC00mIsLJRQv",
        "outputId": "c5ec96c5-4510-4470-b094-6a94e491d1d3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n",
            "Your Kaggle username: rahaf8\n",
            "Your Kaggle Key: ··········\n",
            "Downloading dataset.zip to ./dataset\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 552M/552M [00:32<00:00, 18.0MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_name = \"saharyatimi/datasett\"\n",
        "dataset_dir = \"/content/dataset/dataset\"\n",
        "\n",
        "#get_kaggle_dataset(dataset_name)"
      ],
      "metadata": {
        "id": "VsdinTHK4R9Z"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "video_df = video_dataframe(dataset_dir)\n",
        "classes = video_df['Label'].unique().tolist()\n",
        "print('Number of classes', len(classes))\n",
        "print('Num videos for each class: : ')\n",
        "print(video_df['Label'].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x6e8i--rehke",
        "outputId": "498e213e-16ae-42e6-f0e2-352ecbabdfc6"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of classes 8\n",
            "Num videos for each class: : \n",
            "Stealing       5\n",
            "Vandalism      5\n",
            "Shooting       5\n",
            "Normal         5\n",
            "Robbery        5\n",
            "Shoplifting    5\n",
            "Burglary       5\n",
            "Fighting       5\n",
            "Name: Label, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df, test_df = SplitData(0.2, video_df,classes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xwLcswetdIt",
        "outputId": "030bb665-5097-4d61-f848-ccde59b72ea8"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5 Samples per Class\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ddata = {\"Training\":train_df.groupby(\"Label\").size(),\"Test\":test_df.groupby(\"Label\").size()}\n",
        "\n",
        "ddataframe = pd.DataFrame(data=ddata)\n",
        "ddataframe.plot.bar(stacked= True, rot= 15, title='Training vs Test data',figsize=(15,5))\n",
        "plt.show(block= True)"
      ],
      "metadata": {
        "id": "YVVxLD9721gL"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_destination_dir = '/content/dataset/train'\n",
        "test_destination_dir = '/content/dataset/test'\n",
        "\n",
        "# Move videos to train folders\n",
        "move_videos_to_folders(train_df, train_destination_dir)\n",
        "\n",
        "# Move videos to test folders\n",
        "move_videos_to_folders(test_df, test_destination_dir)\n",
        "\n",
        "#delete empty folders\n",
        "delete_empty_folders('/content/dataset/dataset')"
      ],
      "metadata": {
        "id": "NfbOa2X_6KXs"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_name = '/content/dataset/Filtered_Anomaly_Annotation.txt'\n",
        "batch_size = 8\n",
        "num_frames = 20 # get_minimal_frame_count(file_name)\n",
        "\n",
        "output_signature = (tf.TensorSpec(shape = (None, None, None, 3), dtype = tf.float32),\n",
        "                    tf.TensorSpec(shape = (), dtype = tf.int16))\n",
        "\n",
        "train_ds = tf.data.Dataset.from_generator(\n",
        "    lambda: generate_frames(Path(train_destination_dir), train_df, num_frames, training=True),\n",
        "    output_signature=output_signature\n",
        ")\n",
        "train_ds = train_ds.batch(batch_size)\n",
        "\n",
        "test_ds = tf.data.Dataset.from_generator(\n",
        "    lambda: generate_frames(Path(test_destination_dir), test_df, num_frames),\n",
        "    output_signature=output_signature\n",
        ")\n",
        "test_ds = test_ds.batch(batch_size)"
      ],
      "metadata": {
        "id": "vsWJhNY1292p"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds"
      ],
      "metadata": {
        "id": "lEwWH4y9tKS9",
        "outputId": "364349c4-6b51-45bd-ba23-90e7a2d68beb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_BatchDataset element_spec=(TensorSpec(shape=(None, None, None, None, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int16, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for frames, labels in train_ds.take(5):\n",
        "  print(labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Mo7GDl8-uBD",
        "outputId": "49222a0d-e06c-4b78-b0f0-4e5923d0bccb"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([2 0 1 7 3 5 2 0], shape=(8,), dtype=int16)\n",
            "tf.Tensor([4 4 7 4 2 5 3 6], shape=(8,), dtype=int16)\n",
            "tf.Tensor([7 6 1 4 2 7 3 1], shape=(8,), dtype=int16)\n",
            "tf.Tensor([5 5 3 0 6 6 0 1], shape=(8,), dtype=int16)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Shape: {frames.shape}\")\n",
        "print(f\"Label: {labels.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59LH2qwD-3lB",
        "outputId": "0d94d61e-42bd-485f-eb82-d63225d9b1e0"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape: (8, 20, 320, 320, 3)\n",
            "Label: (8,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## the code below is copied from tensorflow website without any changes - task for tomorrow"
      ],
      "metadata": {
        "id": "bIUEHjiFB6hx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = 'a5'\n",
        "resolution = 320\n",
        "\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "backbone = movinet.Movinet(model_id=model_id)\n",
        "backbone.trainable = False\n",
        "\n",
        "# Set num_classes=600 to load the pre-trained weights from the original model\n",
        "model = movinet_model.MovinetClassifier(backbone=backbone, num_classes=600)\n",
        "model.build([None, None, None, None, 3])\n",
        "\n",
        "# Load pre-trained weights\n",
        "!wget https://storage.googleapis.com/tf_model_garden/vision/movinet/movinet_a0_base.tar.gz -O movinet_a0_base.tar.gz -q\n",
        "!tar -xvf movinet_a0_base.tar.gz\n",
        "\n",
        "checkpoint_dir = f'movinet_{model_id}_base'\n",
        "checkpoint_path = tf.train.latest_checkpoint(checkpoint_dir)\n",
        "checkpoint = tf.train.Checkpoint(model=model)\n",
        "status = checkpoint.restore(checkpoint_path)\n",
        "#status.assert_existing_objects_matched()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D7VoRSxHB-2c",
        "outputId": "da3abfb7-04b0-42a8-fe93-74ba29a7a15b"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "movinet_a0_base/\n",
            "movinet_a0_base/checkpoint\n",
            "movinet_a0_base/ckpt-1.data-00000-of-00001\n",
            "movinet_a0_base/ckpt-1.index\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_classifier(batch_size, num_frames, resolution, backbone, num_classes):\n",
        "  \"\"\"Builds a classifier on top of a backbone model.\"\"\"\n",
        "  model = movinet_model.MovinetClassifier(\n",
        "      backbone=backbone,\n",
        "      num_classes=num_classes)\n",
        "  model.build([batch_size, num_frames, resolution, resolution, 3])\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "oJ4GS9QTCA4r"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_classifier(batch_size, num_frames, resolution, backbone, 8)"
      ],
      "metadata": {
        "id": "4QGjks51CCIC"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 15\n",
        "\n",
        "loss_obj = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate = 0.01)\n",
        "\n",
        "model.compile(loss=loss_obj, optimizer=optimizer, metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "IC2TAm69CDoZ"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = model.fit(train_ds,\n",
        "                    validation_data=test_ds,\n",
        "                    epochs=num_epochs,\n",
        "                    validation_freq=1,\n",
        "                    verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1-WlgpvoCFVa",
        "outputId": "9f9a9e60-f8d9-48b9-f99b-82e3ebe9cc4a"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "4/4 [==============================] - 159s 20s/step - loss: 7.1444 - accuracy: 0.0938 - val_loss: 7.0324 - val_accuracy: 0.1250\n",
            "Epoch 2/15\n",
            "4/4 [==============================] - 28s 7s/step - loss: 5.6763 - accuracy: 0.1250 - val_loss: 4.3357 - val_accuracy: 0.1250\n",
            "Epoch 3/15\n",
            "4/4 [==============================] - 27s 7s/step - loss: 3.4859 - accuracy: 0.1250 - val_loss: 3.2160 - val_accuracy: 0.0000e+00\n",
            "Epoch 4/15\n",
            "4/4 [==============================] - 27s 7s/step - loss: 3.9667 - accuracy: 0.0938 - val_loss: 4.4046 - val_accuracy: 0.0000e+00\n",
            "Epoch 5/15\n",
            "4/4 [==============================] - 30s 8s/step - loss: 4.1319 - accuracy: 0.0938 - val_loss: 6.2779 - val_accuracy: 0.1250\n",
            "Epoch 6/15\n",
            "4/4 [==============================] - 28s 8s/step - loss: 5.7761 - accuracy: 0.1562 - val_loss: 4.8512 - val_accuracy: 0.1250\n",
            "Epoch 7/15\n",
            "4/4 [==============================] - 27s 7s/step - loss: 4.4848 - accuracy: 0.0938 - val_loss: 5.1244 - val_accuracy: 0.1250\n",
            "Epoch 8/15\n",
            "4/4 [==============================] - 27s 7s/step - loss: 7.4527 - accuracy: 0.0625 - val_loss: 3.2280 - val_accuracy: 0.1250\n",
            "Epoch 9/15\n",
            "4/4 [==============================] - 28s 7s/step - loss: 6.2158 - accuracy: 0.0312 - val_loss: 11.3548 - val_accuracy: 0.1250\n",
            "Epoch 10/15\n",
            "4/4 [==============================] - 27s 7s/step - loss: 10.6233 - accuracy: 0.1250 - val_loss: 11.5191 - val_accuracy: 0.1250\n",
            "Epoch 11/15\n",
            "4/4 [==============================] - 27s 7s/step - loss: 8.3883 - accuracy: 0.1562 - val_loss: 9.3698 - val_accuracy: 0.1250\n",
            "Epoch 12/15\n",
            "4/4 [==============================] - 27s 7s/step - loss: 9.9684 - accuracy: 0.1250 - val_loss: 8.8379 - val_accuracy: 0.1250\n",
            "Epoch 13/15\n",
            "4/4 [==============================] - 27s 7s/step - loss: 7.4426 - accuracy: 0.1250 - val_loss: 9.4776 - val_accuracy: 0.1250\n",
            "Epoch 14/15\n",
            "4/4 [==============================] - 27s 7s/step - loss: 8.6629 - accuracy: 0.0938 - val_loss: 14.2206 - val_accuracy: 0.1250\n",
            "Epoch 15/15\n",
            "4/4 [==============================] - 28s 7s/step - loss: 8.4327 - accuracy: 0.1562 - val_loss: 8.3644 - val_accuracy: 0.1250\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(test_ds, return_dict=True)"
      ],
      "metadata": {
        "id": "W3BlNoCICIme"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_actual_predicted_labels(dataset):\n",
        "  \"\"\"\n",
        "    Create a list of actual ground truth values and the predictions from the model.\n",
        "\n",
        "    Args:\n",
        "      dataset: An iterable data structure, such as a TensorFlow Dataset, with features and labels.\n",
        "\n",
        "    Return:\n",
        "      Ground truth and predicted values for a particular dataset.\n",
        "  \"\"\"\n",
        "  actual = [labels for _, labels in dataset.unbatch()]\n",
        "  predicted = model.predict(dataset)\n",
        "\n",
        "  actual = tf.stack(actual, axis=0)\n",
        "  predicted = tf.concat(predicted, axis=0)\n",
        "  predicted = tf.argmax(predicted, axis=1)\n",
        "\n",
        "  return actual, predicted"
      ],
      "metadata": {
        "id": "k3RE_h_wCLfo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_confusion_matrix(actual, predicted, labels, ds_type):\n",
        "  cm = tf.math.confusion_matrix(actual, predicted)\n",
        "  ax = sns.heatmap(cm, annot=True, fmt='g')\n",
        "  sns.set(rc={'figure.figsize':(12, 12)})\n",
        "  sns.set(font_scale=1.4)\n",
        "  ax.set_title('Confusion matrix of action recognition for ' + ds_type)\n",
        "  ax.set_xlabel('Predicted Action')\n",
        "  ax.set_ylabel('Actual Action')\n",
        "  plt.xticks(rotation=90)\n",
        "  plt.yticks(rotation=0)\n",
        "  ax.xaxis.set_ticklabels(labels)\n",
        "  ax.yaxis.set_ticklabels(labels)"
      ],
      "metadata": {
        "id": "eqzk3JWmKG4X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fg = FrameGenerator(Path(train_destination_dir), num_frames, training = True)\n",
        "label_names = list(fg.class_ids_for_name.keys())"
      ],
      "metadata": {
        "id": "4f2al16ECQi-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "actual, predicted = get_actual_predicted_labels(test_ds)"
      ],
      "metadata": {
        "id": "B1l5S1kVCR7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'MobileNet Model accuracy on the test set is : {accuracy_score(actual, predicted )*100:.2f}%')"
      ],
      "metadata": {
        "id": "pFh8oMy7LlCV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dphxtosHDDtJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dqKaxgeKhEka"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}